[toc]
# 一、机器学习理论
## （零) 重点关注
### 1. 监督学习&无监督学习
#### 1.1 监督学习（supervised learning）
从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型，再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）。

监督学习是训练神经网络和决策树的常见技术。这两种技术高度依赖事先确定的分类系统给出的信息，对于神经网络，分类系统利用信息判断网络的错误，然后不断调整网络参数。对于决策树，分类系统用它来判断哪些属性提供了最多的信息。

> 有监督学习最常见的就是：regression&classification
  * Regression：Y是实数vector。回归问题，就是拟合(x,y)的一条曲线，使得价值函数(costfunction) L最小
  * Classification：Y是一个有穷数(finitenumber)，可以看做类标号，分类问题首先要给定有lable的数据训练分类器，故属于有监督学习过程。分类过程中cost function l(X,Y)是X属于类Y的概率的负对数。

#### 1.2 无监督学习（unsupervised learning）
输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

> 无监督学习的方法分为两大类：
* 一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。
* 另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。
利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。

### 2. 分类器和预测器
#### 2.1 预测器
将计算机看成一个机器，输入一个数得到输出（假设函数是y=kx最简单的线性函数）。那么我们如何使用这个函数由输入预测输出呢？答案要从过去中来。如果我们知道一组数据并且想要发现这组输入输出数据的关系从而达到预测的目的那么我们需要不断地比较已有输出和预测输出之间的误差，并且设计一个合理地调节梯度。通过多次训练最终拟合出最合适的函数（即误差最小）。
#### 2.2 梯度
调整的幅度即为梯度。如预测的输出大于实际输出，那么我们可以调整k使其增加0.1，调整的幅度即为梯度。调整幅度的过程涉及到机器学习中至关重要的概念学习率：ΔK=L(E/x)。这里E/x即(y-y’)/k只是对于上文提到的函数y=kx而言，即每次调整K的幅度应该是不断的迭代（E为误差值即E=y-y’预测值与实际值之间的误差，L标记为学习率由我们设置如L=0.5每次只学习预期调整幅度的一半）。这样做的好处是避免训练数据本身的不确定性所带来的误差，可以有节制地抑制某些极端样本，更有利于还原整体情况。
##### 2.2.1 数学意义
在微积分里面，对多元函数的参数求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

##### 2.2.2 几何意义：
函数变化增加最快的地方。具体来说，对于函数,在点，沿着梯度向量的方向就是的方向是增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 的方向，梯度减少最快，也就是更加容易找到函数的最小值。
##### 2.2.3 个人理解
梯度指向值变化最大的方向！所以负梯度就是指向最优权重。

#### 2.3 分类器
简单的线性函数能帮我们起到分类的作用。显然一条直线将整个坐标平面分为了上下两个部分，即分为了两类。一个有趣的例子可以证明这个极简单的分类器的功效。
花园里有毛毛虫和瓢虫，毛毛虫很长而瓢虫却很宽，如果我们把长和宽作为x与Y放入坐标轴中就可以通过该坐标在直线的上部或下部轻松地区别出毛毛虫与瓢虫即分类的功能得到实现（然后很快我们就发现该分类器是线性的无法实现一些相对复杂的功能。如：XOR逻辑功能，此时我们需要多个分类器辅助我们完成这个功能）。

### 3. 独热码
#### 3.1 what
独热编码（One-Hot Encoding），又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，其中只有一位有效。即，只有一位是1，其余都是零值。

> 例如，对六个状态进行编码：
自然顺序码为 000,001,010,011,100,101
独热编码则是 000001,000010,000100,001000,010000,100000

#### 3.2 why

在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的。而常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。
使用独热编码（One-Hot Encoding），将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用独热编码（One-Hot Encoding），会让特征之间的距离计算更加合理。

### 4. 向量化
#### 4.1 what
将数据以向量的形式输入。
#### 4.2 why
一言蔽之，**向量化快！**
向量化可以去除代码中 的for 循环。当在深度学习领域，代码中显式地使用 for 循环使算法很低效，如果在大数据集上，代码可能花费很长时间去运行，
向量化的实现将会非常直接计算 wT*x 。使用两个方法——向量化和非向量化，计算相同的值，其中向量化版本花费了0.968毫秒，而非向量化版本的 for 循环花费了327.997毫秒，大概是300多倍。
### 5.学习率
#### 5.1 梯度
见2.2
#### 5.2 学习率
“下山步子”的大小。
#### 5.3 损失函数(Loss Function 或 Cost Function)
代价函数也称为损失函数。代价函数并没有准确的定义，可以理解为是一个人为定义的函数，我们可以利用这个函数来优化模型的参数。最简单且常见的一个代价函数是均方差(MSE)代价函数，也称为二次代价函数。（就是方差）
#### 5.4 过拟合
[过拟合][18] ：简单来说，就是训练的函数不适用于测试集

### 6. 神经元
神经元模型模拟大脑神经元的运行过程，其包含输入，输出与计算功能，输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。下图是一个典型的神经元模型：包含有m个输入，1个输出，以及2个计算功能。
![神经元](https://s3.bmp.ovh/imgs/2022/10/14/fcac82b398f61973.jpg)
## （一）神经网络
### 0. 分类
神经网络可以分为三种主要类型：前馈神经网络、反馈神经网络和图神经网络。

### 1. 前馈神经网络
前馈神经网络（feedforward neural network）是一种简单的神经网络，也被称为多层感知机（multi-layer perceptron，简称MLP），其中不同的神经元属于不同的层，由输入层-隐藏层-输出层构成，信号从输入层往输出层单向传递，中间无反馈。

图1 前馈神经网络结构图
![文字替代图片](图片地址)
#### 1.1 组成
前馈神经网络中包含激活函数（sigmoid函数、tanh函数等）、损失函数（均方差损失函数、交叉熵损失函数等）、优化算法（BP算法）等。
#### 1.2 例子
常用的模型结构有：卷积神经网络、BP神经网络、RBF神经网络、感知器网络等。
#### 1.3 实例：卷积神经网络 CNN
[卷积神经网络][12]（Convolutional Neural Networks, CNN）是一类包含卷积运算且具有深度结构的前馈神经网络（Feedforward Neural Networks）。
##### 1.3.0 what
整体架构：输入层—卷积层—池化层—全连接层—输出层

##### 1.3.1 输入层
以图片为例，输入的是一个三维像素矩阵，长和宽表示图像的像素大小，深度表示色彩通道（黑白为1，RGB彩色为3）。

##### 1.3.2 卷积层
卷积层也是一个三维矩阵，它的每个节点（单位节点矩阵）都是上一层的一小块节点（子节点矩阵）加权得来，一小块的尺寸一般取3*3或5*5。此层的作用是对每一小快节点进行深入分析，从而提取图片更高的特征。

##### 1.3.3 池化层
池化层不会改变三维矩阵的深度，其作用是缩小矩阵，从而减少网络的参数。

##### 1.3.4 全连接层
跟**全连接神经网络**作用一样。
##### 1.3.5 激活层
负责对卷积层抽取的特诊进行激活，由于卷积操作是把输入图像和卷积核进行相应的线性变换，需要引入激活层(非线性函数)对其进行非线性映射。激活层由非线性函数组成，常见的如sigmoid、tanh、relu。最常用的激活函数是Relu，又叫线性整流器。
  * **why?**  线性函数没有上界，经常会造成一个节点处的数字变得很大很大，难以计算，也就无法得到一个可以用的网络。因此人们后来对节点上的数据进行了一个操作，如利用sigmoid()函数来处理，使数据被限定在一定范围内。

* Softmax层：得到当前样例属于不同种类的概率分布，并完成分类。

##### 1.3.6 why
相比早期的BP神经网络，卷积神经网络最重要的特性在于“参数共享”与“局部感知”。
* 权重共享
传统的神经网络的参数量巨大，***全连接神经网络*** 最大的问题就是权值参数太多，而卷积神经网络的卷积层，不同神经元的权值是共享的，这使得整个神经网络的参数大大减小，提高了整个网络的训练性能。
* 局部感知
一张图像，我们实际上并不需要让每个神经元都接受整个图片的信息，而是让不同区域的神经元对应一整张图片的不同局部，最后只要再把局部信息整合到一起就可以了。这样就相当于在神经元最初的输入层实现了一次降维
* 平移不变性
即使图片中的目标位置平移到图片另一个地方，卷积神经网络仍然能很好地识别出这个目标，输出的结果也和原来未平移之前是一致的。
##### 1.3.7 卷积
[卷积的本质：先将一个函数翻转，然后进行滑动叠加。][13]
#### 1.4 BP神经网络
[Bp神经网络][11]可以**分为两个部分**，bp和神经网络。bp是 Back Propagation 的简写，意思是反向传播。
> 一个通俗的例子，猜数字：
提前设定一个数值 50，通过猜的数字是高了还是低了得到正确答案。


### 2. 反馈神经网络
反馈神经网络（feedback neural network）的输出不仅与当前输入以及网络权重有关，还和网络之前的输入有关。常用的模型结构有：RNN、Hopfield网络、玻尔兹曼机、LSTM等。

![文字替代图片](图片地址)
图3 反馈神经网络结构图

#### 2.1 实例：循环神经网络 RNN：
> 在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题都无能无力。比如你要预测句子的下一个单词是什么，一般需要用到前面的单词。

RNN之所以称为循环神经网络，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即**隐藏层之间的节点不再无连接而是有连接的**，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。
![RNN神经网络的结构](图片地址)

### 3. 图神经网络
图（graph）是一种在拓扑空间内按图结构组织来关系推理的函数集合，包括社交网络、知识图谱、分子图神经网络等。
GNN是直接在图数据结构上运行的神经网络。GNN的典型应用便是节点分类。
相比较于神经网络最基本的网络结构全连接层（MLP），特征矩阵乘以权重矩阵，图神经网络多了一个邻接矩阵。计算形式很简单，三个矩阵相乘再加上一个非线性变换：

## （二） 如果能重来，我会按这个顺序学习：
[1.先用最快时间看吴恩达讲理论,][14]
[2.卷积神经网络图像化理解(从52p开始),][15]
[3.零基础，理论+实操的帖子][16]+[4.莫烦，实操的视频][17]
[5.再看李沐的巩固一遍理论和实操][17]


[18]:https://blog.csdn.net/u012950413/article/details/80376136?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166548733916800184187762%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166548733916800184187762&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-80376136-null-null.142^v52^control,201^v3^control_2&utm_term=%E8%BF%87%E6%8B%9F%E5%90%88%E6%98%AF%E4%BB%80%E4%B9%88&spm=1018.2226.3001.4187

[11]:https://blog.csdn.net/weixin_40432828/article/details/82192709

[12]:https://blog.csdn.net/weixin_39784263/article/details/109957071

[13]:https://www.zhihu.com/question/22298352/answer/637156871

[14]:https://www.bilibili.com/video/BV164411b7dx

[15]:https://www.bilibili.com/video/BV1we4y1X7vy

[16]:https://zhuanlan.zhihu.com/p/377513272

[17]:https://www.bilibili.com/video/BV1if4y147hS
